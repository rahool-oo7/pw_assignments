{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ASSIGNMENT - ENSEMBLE LEARNING :**"
      ],
      "metadata": {
        "id": "SLxGy-Buhugr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 1:*  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Ensemble learning is a method where multiuple models ar ecombined to make a final prediction.\n",
        "The key idea, many weak models perform better than one strong model.\n",
        "---\n",
        "*Question 2:* What is the difference between Bagging and Boosting?\n",
        "- Bagging: Models are trained independently on random subset of data and reduces variance and avoid overfitting\n",
        "- Boosting: Models are trained one after another, each model focuses more on The previous model error, therefore, reduces bias and increases accuracy.\n",
        "---\n",
        "*Question 3:* What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "- Bootstrap sampling means randomly selecting data with replacement to create new training subsets. In bagging, each model gets a different dataset, which makes them diverse and improves the final prediction.\n",
        "---\n",
        "*Question 4:* What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "- When taking random subsets of data using Bootstrap sampling, some datapoints that are not selected are called OOB samples, and the model predicts these OOB points, and the accuracy of these predictions is the OOB score. This act as a built-in validation accuracy.\n",
        "---\n",
        "*Question 5:* Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "- Single DT: Feature importance depends on how much each feature reduces impurity in that one tree. It may be unstable because trees are sensitive to small data changes.\n",
        "- Random Forest: Importance is averaged across many trees, making it more stable, more reliable, and less biased.\n",
        "---\n"
      ],
      "metadata": {
        "id": "n9A9iRIHhjkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 6:*  Write a Python program to:\n",
        "- ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "- ● Train a Random Forest Classifier\n",
        "- ● Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "42MDqaUktzxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "importance = rf.feature_importances_\n",
        "top_5 = np.argsort(importance)[-5:][::-1]\n",
        "for i in top_5:\n",
        "  print(data.feature_names[i],\": \", importance[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTWSp8HVt1Ve",
        "outputId": "d7ddc077-91e1-41b4-b64a-96118e47ec18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst perimeter :  0.1934404224740546\n",
            "worst area :  0.12650990229995993\n",
            "worst concave points :  0.09057614572233937\n",
            "worst radius :  0.0795149390211507\n",
            "mean concave points :  0.06788280003738885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 7:* Write a Python program to:\n",
        "- ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "- ● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "vJ-OpFE1t1lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "acc_1 = dt.score(X_test, y_test)\n",
        "print(\"Decision Tree Accuracy: \", acc_1)\n",
        "\n",
        "bagg = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=21, oob_score=True)\n",
        "bagg.fit(X_train, y_train)\n",
        "acc_2 = bagg.score(X_test, y_test)\n",
        "print(\"Bagging Accuracy: \", acc_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56L83Hrgt6-r",
        "outputId": "61dadf15-4769-4ed3-cd7c-62b716d73b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy:  0.8666666666666667\n",
            "Bagging Accuracy:  0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 8:* Write a Python program to:\n",
        "- ● Train a Random Forest Classifier\n",
        "- ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "- ● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "hkN1b3s1t7M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "param = {'max_depth': [5,10,15,20,25], 'n_estimators': [10,25,30,50,100]}\n",
        "grid = GridSearchCV(RandomForestClassifier(), param, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters :\", grid.best_params_)\n",
        "print(\"Best Score :\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m759dyv1t_uy",
        "outputId": "d81ef916-55b0-46c1-8131-a75002b87c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters : {'max_depth': 20, 'n_estimators': 50}\n",
            "Best Score : 0.9626373626373625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 9:*  Write a Python program to:\n",
        "- ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "- ● Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "NYBpeYXruCki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "print(\"Random Forest MSE: \", rf_mse)\n",
        "\n",
        "bagg = BaggingRegressor()\n",
        "bagg.fit(X_train, y_train)\n",
        "bagg_mse = mean_squared_error(y_test, bagg.predict(X_test))\n",
        "print(\"Bagging mse: \", bagg_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4TIdM3TuEXq",
        "outputId": "63693c9c-5485-4633-d8ee-c3c2b6e1da57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest MSE:  0.25602601887225224\n",
            "Bagging mse:  0.2756948709699583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 10:* You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "- ● Choose between Bagging or Boosting\n",
        "- ● Handle overfitting\n",
        "- ● Select base models\n",
        "- ● Evaluate performance using cross-validation\n",
        "- ● Justify how ensemble learning improves decision-making in this real-world\n",
        "context"
      ],
      "metadata": {
        "id": "UpZyVdlXuEmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ANSWER:\n",
        "\n",
        "- If we have a cleaned dataset, use Boosting and if the current model is overfitting, use Bagging as it reduces variance and helps stabilise the model. Boosting is preferred for loan default prediction as it focuses on correcting previous errors and improves prediction accuracy on complex patterns.\n",
        "- Overfitting -> Use regularization, shallow trees, early stopping, and cross-validation to prevent overfitting.\n",
        "- Base model -> Decision Trees are used as base learners since they handle non-linearity and mixed data effectively\n",
        "- Performance -> Apply K-fold cross-validation and evaluate using ROC-AUC, F1-score, and Precision-Recall metrics.\n",
        "- Ensemble models reduce individual model errors, improve risk prediction, and support better loan approval decisions."
      ],
      "metadata": {
        "id": "rsMtvJ6EJA6w"
      }
    }
  ]
}